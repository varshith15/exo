{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"~/.cache/huggingface/hub/models--mlx-community--Meta-Llama-3-8B-Instruct-4bit/snapshots/c38b3b1f03cce0ce0ccd235e5c97b0d3d255e651\").expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path/\"config.json\", \"r\") as f:\n",
    "  config = json.load(f)\n",
    "\n",
    "print(config[\"quantization\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_files = glob.glob(str(model_path/\"model*.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight = mx.load(weight_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight[\"model.layers.0.self_attn.q_proj.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight[\"model.layers.0.self_attn.q_proj.scales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weight[\"model.layers.0.self_attn.q_proj.biases\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exo.inference.mlx.models.llama import LlamaModel, ModelArgs\n",
    "\n",
    "args = ModelArgs.from_dict(config)\n",
    "model = LlamaModel(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = model.leaf_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx.utils import tree_map_with_path\n",
    "from mlx.nn.layers.base import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_predicate(p, m):\n",
    "    # print(m, hasattr(m, \"to_quantized\"), f\"{p}.scales\", f\"{p}.scales\" in temp_weight)\n",
    "    if not hasattr(m, \"to_quantized\"):\n",
    "        return False\n",
    "    return f\"model.{p}.scales\" in temp_weight\n",
    "\n",
    "def _maybe_quantize(path, m):\n",
    "    if class_predicate(path, m):\n",
    "        # print(\"hahahaha\")\n",
    "        if hasattr(m, \"to_quantized\"):\n",
    "            # print(\"hahahaha\")\n",
    "            k = m.to_quantized(64, 4)\n",
    "            print(k)\n",
    "            return k\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to quantize model of type {type(m)}\")\n",
    "    else:\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_leaves = tree_map_with_path(_maybe_quantize, leaves, is_leaf=Module.is_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves[\"layers\"][0][\"self_attn\"][\"o_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_leaves[\"layers\"][0][\"self_attn\"][\"o_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = nn.Linear(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(ll, \"to_quantized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-2-7b-bnb-4bit\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/llama-2-7b-bnb-4bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = set()\n",
    "for t in temp_weight.keys():\n",
    "    if \"scales\" in t:\n",
    "        if \".\".join(t.split(\".\")[3:-1]) == \"\":\n",
    "            print(t)\n",
    "        ans.add(\".\".join(t.split(\".\")[3:-1]))\n",
    "\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = nn.Conv2d(3, 1024, 16, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlavaForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image    \n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_URLS = [\n",
    "    \"https://picsum.photos/id/237/400/300\",\n",
    "    \"https://picsum.photos/id/231/200/300\",\n",
    "    \"https://picsum.photos/id/27/500/500\",\n",
    "    \"https://picsum.photos/id/17/150/600\",\n",
    "]\n",
    "PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n",
    "\n",
    "inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs[\"pixel_values\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = mx.ones((20,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.multiply(ll, ll) == ll * ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.multiply(ll, ll).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = ll * ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = 1.0 / (1000000000.0 ** (mx.arange(0, 1024, 2) / 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = mx.arange(1024//16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_h = mx.outer(h, freqs[::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_h.repeat(1, 1024//16, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.tile(freqs_h[: None, :], (1, 1024//16, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = torch.ones((1, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"hf-internal-testing/pixtral-12b\"\n",
    "hf_model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model.save_pretrained(\"./../mistral_weights/\",  save_peft_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_tower.save_pretrained(\"./../pixtral_weights/\",  save_peft_format=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in model.language_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "IMG_URLS = [\n",
    "    \"https://picsum.photos/id/237/400/300\",\n",
    "    \"https://picsum.photos/id/231/200/300\",\n",
    "    \"https://picsum.photos/id/27/500/500\",\n",
    "    \"https://picsum.photos/id/17/150/600\",\n",
    "]\n",
    "PROMPT = \"<s>[INST]Describe the images in one sentence.\\n[IMG][IMG][IMG][IMG][/INST]\"\n",
    "\n",
    "inputs = processor(images=IMG_URLS, text=PROMPT, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_ids = model.language_model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouptut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.save_pretrained(\"./../mistral_weights/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.language_model.forward(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model.model.layers[10].mlp.gate_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model.model.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model.model.embed_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vision_tower.forward(inputs[\"pixel_values\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "# from exo.inference.mlx.models.pixtral import PixtralModel, PixtralVisionConfig\n",
    "weights = {}\n",
    "weights.update(mx.load(\"./../pixtral_weights/model.safetensors\"))\n",
    "\n",
    "# import json\n",
    "# with open(\"./../pixtral_weights/config.json\", \"r\") as f:\n",
    "#     config = json.load(f)\n",
    "# vision_config = PixtralVisionConfig.from_dict(config)\n",
    "# model = PixtralModel(vision_config)\n",
    "# sanitized_weights = model.sanitize(weights)\n",
    "# model.load_weights(list(sanitized_weights.items()), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_max = -10000000\n",
    "_min = 10000000\n",
    "for key in weights:\n",
    "    _max = max(_max, weights[key].max())\n",
    "    _min = min(_min, weights[key].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_max, _min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[\"transformer.layers.9.ffn_norm.weight\"].astype(mx.float16).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[\"transformer.layers.9.ffn_norm.weight\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.vision_tower.forward(inputs[\"pixel_values\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_vals = [mx.array(x) for x in inputs[\"pixel_values\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(pixel_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in weights.items():\n",
    "    if \"patch_conv\" in key:\n",
    "        print(weights[key].transpose(0, 2, 3, 1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.vision_tower.patch_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.patch_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.patch_conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model.vision_tower.patch_conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embeds_list = [hf_model.vision_tower.patch_conv(img.unsqueeze(0).to(torch.bfloat16)) for img in inputs[\"pixel_values\"][0]]\n",
    "patch_embeds = torch.cat([p.flatten(2).permute(0, 2, 1) for p in patch_embeds_list], dim=1)\n",
    "patch_embeds = hf_model.vision_tower.ln_pre(patch_embeds)\n",
    "\n",
    "# hf_generate_block_attention_mask([p.shape[-2] * p.shape[-1] for p in patch_embeds_list], patch_embeds)\n",
    "print(patch_embeds_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "patch_embeds_list = [model.patch_conv(mx.expand_dims(mx.array(img), axis=0).transpose(0, 2, 3, 1)) for img in inputs[\"pixel_values\"][0]]\n",
    "\n",
    "patch_embeds = mx.concatenate([p.flatten(1, 2) for p in patch_embeds_list], axis=1)\n",
    "patch_embeds = model.ln_pre(patch_embeds)\n",
    "\n",
    "# print(patch_embeds_list[0].shape)\n",
    "\n",
    "mlx_generate_block_attention_mask(\n",
    "            [p.shape[1] * p.shape[2] for p in patch_embeds_list], patch_embeds\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_generate_block_attention_mask(patch_embeds_list, tensor):\n",
    "    dtype = tensor.dtype\n",
    "    device = tensor.device\n",
    "    seq_len = tensor.shape[1]\n",
    "    d_min = torch.finfo(dtype).min\n",
    "    causal_mask = torch.full((seq_len, seq_len), fill_value=d_min, dtype=dtype, device=device)\n",
    "\n",
    "    block_end_idx = torch.tensor(patch_embeds_list).cumsum(-1)\n",
    "    block_start_idx = torch.tensor([0] + patch_embeds_list[:-1]).cumsum(-1)\n",
    "    for start, end in zip(block_start_idx, block_end_idx):\n",
    "        print(start, end)\n",
    "        causal_mask[start:end, start:end] = 0\n",
    "\n",
    "    causal_mask = causal_mask[None, None, :, :].expand(tensor.shape[0], 1, -1, -1)\n",
    "    return causal_mask\n",
    "\n",
    "def mlx_generate_block_attention_mask(patch_embeds_list, input_array):\n",
    "    dtype = np.array(input_array).dtype\n",
    "    seq_len = input_array.shape[1]\n",
    "    dmin = np.finfo(dtype).min\n",
    "    causal_mask = mx.full((seq_len, seq_len), dmin, dtype=input_array.dtype)\n",
    "\n",
    "    block_end_idx = mx.cumsum(mx.array(patch_embeds_list), axis=-1).tolist()\n",
    "    block_start_idx = mx.cumsum(mx.array([0] + patch_embeds_list[:-1]), axis=-1).tolist()\n",
    "    \n",
    "    for start, end in zip(block_start_idx, block_end_idx):\n",
    "        print(start, end)\n",
    "        causal_mask[start:end, start:end] = 0\n",
    "\n",
    "    causal_mask = mx.broadcast_to(causal_mask[None, None, :, :], (input_array.shape[0], 1, seq_len, seq_len))\n",
    "    return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_conv = model.patch_conv(mx.expand_dims(mx.array(inputs[\"pixel_values\"][0][0]), axis=0).transpose(0, 2, 3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_conv\n",
    "patch_embeds = torch.cat([p.flatten(2).permute(0, 2, 1) for p in [torch_conv]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.array(torch_conv.flatten(2).permute(0, 2, 1).to(torch.float32).detach().numpy()) == mlx_conv.flatten(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_conv.flatten(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_result = mx.array(torch_conv.flatten(2).permute(0, 2, 1).detach().numpy())\n",
    "mlx_result = mlx_conv.flatten(1, 2)\n",
    "\n",
    "are_close = mx.allclose(torch_result, mlx_result, atol=1e-4, rtol=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = mx.abs(torch_result - mlx_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_position_ids_in_meshgrid(patch_embeds_list, max_width):\n",
    "    positions = []\n",
    "    for patch in patch_embeds_list:\n",
    "        height, width = patch.shape[-2:]\n",
    "        mesh = torch.meshgrid(torch.arange(height), torch.arange(width), indexing=\"ij\")\n",
    "        # return torch.stack(mesh, dim=-1).reshape(-1, 2)\n",
    "        h_grid, v_grid = torch.stack(mesh, dim=-1).reshape(-1, 2).chunk(2, -1)\n",
    "        ids = h_grid * max_width + v_grid\n",
    "        # return ids\n",
    "        positions.append(ids[:, 0])\n",
    "    return torch.cat(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_position_ids_in_meshgrid([torch_conv], 1024//16).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlx_position_ids_in_meshgrid(patch_embeds_list, max_width):\n",
    "    positions = []\n",
    "    for patch in patch_embeds_list:\n",
    "        height, width = patch.shape[1:3]\n",
    "        mesh = mx.meshgrid(mx.arange(height), mx.arange(width), indexing=\"ij\")\n",
    "        h_grid, v_grid = mesh[0].reshape(-1), mesh[1].reshape(-1)\n",
    "        ids = h_grid * max_width + v_grid\n",
    "        positions.append(ids)\n",
    "    return mx.concatenate(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_position_ids = mlx_position_ids_in_meshgrid([mlx_conv], 1024//16)\n",
    "mlx_position_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_position_ids = torch_position_ids_in_meshgrid([torch_conv], 1024//16)\n",
    "torch_position_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(kp) == jp.detach().numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.expand_dims(mx.array(inputs[\"pixel_values\"][0][0]), axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"pixel_values\"][0][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embeds = torch.cat([p.flatten(2).permute(0, 2, 1) for p in [torch_conv]], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_position_embedding = hf_model.vision_tower.patch_positional_embedding(patch_embeds, torch_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_position_embedding[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_position_embedding[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_position_embeddings = model.patch_positional_embedding(\"1\", mlx_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_position_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_position_embedding[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_position_embeddings[0] == hf_position_embedding[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_generate_block_attention_mask(patch_embeds_list, tensor):\n",
    "    dtype = tensor.dtype\n",
    "    device = tensor.device\n",
    "    seq_len = tensor.shape[1]\n",
    "    d_min = torch.finfo(dtype).min\n",
    "    causal_mask = torch.full((seq_len, seq_len), fill_value=d_min, dtype=dtype, device=device)\n",
    "\n",
    "    block_end_idx = torch.tensor(patch_embeds_list).cumsum(-1)\n",
    "    block_start_idx = torch.tensor([0] + patch_embeds_list[:-1]).cumsum(-1)\n",
    "    for start, end in zip(block_start_idx, block_end_idx):\n",
    "        causal_mask[start:end, start:end] = 0\n",
    "\n",
    "    causal_mask = causal_mask[None, None, :, :].expand(tensor.shape[0], 1, -1, -1)\n",
    "    return causal_mask\n",
    "\n",
    "def mlx_generate_block_attention_mask(patch_embeds_list, input_array):\n",
    "    dtype = np.array(input_array).dtype\n",
    "    seq_len = input_array.shape[1]\n",
    "    dmin = np.finfo(dtype).min\n",
    "    causal_mask = mx.full((seq_len, seq_len), dmin, dtype=input_array.dtype)\n",
    "\n",
    "    block_end_idx = mx.cumsum(mx.array(patch_embeds_list), axis=-1).tolist()\n",
    "    block_start_idx = mx.cumsum(mx.array([0] + patch_embeds_list[:-1]), axis=-1).tolist()\n",
    "    \n",
    "    for start, end in zip(block_start_idx, block_end_idx):\n",
    "        causal_mask[start:end, start:end] = 0\n",
    "\n",
    "    causal_mask = mx.broadcast_to(causal_mask[None, None, :, :], (input_array.shape[0], 1, seq_len, seq_len))\n",
    "    return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlx_generate_block_attention_mask(patch_embeds_list, input_array):\n",
    "    dtype = np.array(input_array).dtype\n",
    "    seq_len = input_array.shape[1]\n",
    "    dmin = np.finfo(dtype).min\n",
    "    causal_mask = mx.full((seq_len, seq_len), dmin, dtype=input_array.dtype)\n",
    "\n",
    "    block_end_idx = mx.cumsum(mx.array(patch_embeds_list), axis=-1).tolist()\n",
    "    block_start_idx = mx.cumsum(mx.array([0] + patch_embeds_list[:-1]), axis=-1).tolist()\n",
    "    \n",
    "    for start, end in zip(block_start_idx, block_end_idx):\n",
    "        causal_mask[start:end, start:end] = 0\n",
    "\n",
    "    causal_mask = mx.broadcast_to(causal_mask[None, None, :, :], (input_array.shape[0], 1, seq_len, seq_len))\n",
    "    return causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = hf_generate_block_attention_mask(\n",
    "            [p.shape[-2] * p.shape[-1] for p in [torch_conv]], patch_embeds\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_patch_embeds = mx.concatenate([p.flatten(1, 2) for p in [mlx_conv]], axis=1)\n",
    "kp = mlx_generate_block_attention_mask(\n",
    "            [p.shape[-2] * p.shape[-1] for p in [mlx_conv]], mlx_patch_embeds\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp.detach().numpy() == kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(jp.to(torch.float32).detach().numpy() == kp).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlavaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"hf-internal-testing/pixtral-12b\"\n",
    "hf_model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "IMG_URLS = [\n",
    "    \"https://picsum.photos/id/237/400/300\",\n",
    "    \"https://picsum.photos/id/231/200/300\",\n",
    "    \"https://picsum.photos/id/27/500/500\",\n",
    "    \"https://picsum.photos/id/17/150/600\",\n",
    "]\n",
    "PROMPT = \"<s>[INST]Describe the images in one sentence.\\n[IMG][/INST]\"\n",
    "\n",
    "inputs = processor(PROMPT, IMG_URLS[:1], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = hf_model.forward(input_ids=inputs[\"input_ids\"], pixel_values=inputs[\"pixel_values\"][0], attention_mask=inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = hf_model.vision_tower.forward(inputs[\"pixel_values\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from exo.inference.mlx.models.pixtral import PixtralModel, PixtralVisionConfig\n",
    "weights = {}\n",
    "weights.update(mx.load(\"./../pixtral_weights/model.safetensors\"))\n",
    "\n",
    "import json\n",
    "with open(\"./../pixtral_weights/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "vision_config = PixtralVisionConfig.from_dict(config)\n",
    "model = PixtralModel(vision_config)\n",
    "sanitized_weights = model.sanitize(weights)\n",
    "model.load_weights(list(sanitized_weights.items()), strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_vals = [mx.array(x) for x in inputs[\"pixel_values\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = hf_model.vision_tower.forward(inputs[\"pixel_values\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = model(pixel_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.finfo(torch.bfloat16).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float('-inf') + 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the arrays\n",
    "self = np.zeros((1, 5), dtype=int)\n",
    "mask = np.array([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], dtype=bool)\n",
    "source = np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
    "\n",
    "# Use boolean indexing to perform the masked scatter\n",
    "self[mask] = source[mask]\n",
    "\n",
    "# indices = mx.argwhere(special_image_mask)\n",
    "#     inputs_embeds = mx.scatter(inputs_embeds, indices, image_features.reshape(-1, image_features.shape[-1]))\n",
    "\n",
    "print(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = torch.tensor([0, 0, 0, 0, 0])\n",
    "mask = torch.tensor([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], dtype=torch.bool)\n",
    "source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
    "self.masked_scatter(mask, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "# Initialize the arrays\n",
    "self = mx.zeros((2, 5), dtype=mx.int32)\n",
    "mask = mx.array([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], dtype=mx.bool_)\n",
    "source = mx.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
    "\n",
    "# Create the result array\n",
    "result = mx.zeros_like(self)\n",
    "\n",
    "# Flatten the source array\n",
    "flat_source = source.reshape(-1)\n",
    "\n",
    "# Manually update the result array\n",
    "scatter_index = 0\n",
    "for i in range(mask.shape[0]):\n",
    "    for j in range(mask.shape[1]):\n",
    "        if mask[i, j]:\n",
    "            result = result.at[i, j].add(flat_source[scatter_index])\n",
    "            scatter_index += 1\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "\n",
    "def masked_scatter(self1, mask, source):\n",
    "    \"\"\"\n",
    "    Scatter values from source into self at positions where mask is True.\n",
    "    \n",
    "    Args:\n",
    "    self (mx.array): The array to be modified.\n",
    "    mask (mx.array): A boolean mask of the same shape as self.\n",
    "    source (mx.array): The array containing values to be scattered.\n",
    "    \n",
    "    Returns:\n",
    "    mx.array: A new array with values from source scattered into self where mask is True.\n",
    "    \n",
    "    Note:\n",
    "    This function assumes broadcasting rules similar to PyTorch.\n",
    "    \"\"\"\n",
    "    mask = mx.array(mask, dtype=mx.bool_)\n",
    "    \n",
    "    # Ensure shapes are compatible\n",
    "    if self1.shape != mask.shape:\n",
    "        raise ValueError(\"Shapes of self and mask must be the same\")\n",
    "    \n",
    "    # Flatten arrays\n",
    "    self_flat = self1.reshape(-1)\n",
    "    mask_flat = mask.reshape(-1)\n",
    "    source_flat = source.reshape(-1)\n",
    "    \n",
    "    # Ensure source has enough elements\n",
    "    num_true = int(mx.sum(mask_flat))\n",
    "    if source_flat.size < num_true:\n",
    "        raise ValueError(\"Source array does not have enough elements to scatter.\")\n",
    "    \n",
    "    # Create output array\n",
    "    output_flat = mx.where(mask_flat, source_flat[:num_true], self_flat)\n",
    "    \n",
    "    # Reshape output to original shape\n",
    "    return output_flat.reshape(self.shape)\n",
    "\n",
    "# Example usage\n",
    "self1 = mx.zeros(5)\n",
    "mask = mx.array([[False, False, False, True, True],\n",
    "                    [True, True, False, True, True]], dtype=mx.bool_)\n",
    "source = mx.array([[0, 1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8, 9]])\n",
    "\n",
    "result = masked_scatter(self1, mask, source)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = torch.tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\n",
    "mask = torch.tensor([[1, 1, 1, 1, 1], [1, 0, 1, 1, 1]], dtype=torch.bool)\n",
    "source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
    "self.masked_scatter(mask, source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_scatter(inputs_embeds, special_image_mask, image_features):\n",
    "    # Flatten all arrays\n",
    "    flat_result = inputs_embeds.reshape(-1)\n",
    "    flat_mask = special_image_mask.reshape(-1)\n",
    "    flat_source = image_features.reshape(-1)\n",
    "    \n",
    "    source_idx = 0\n",
    "    for i in range(flat_result.size()[0]):\n",
    "        if flat_mask[i]:\n",
    "            flat_result[i] = flat_source[source_idx]\n",
    "            source_idx += 1\n",
    "                \n",
    "    # Reshape the result back to the original shape\n",
    "    return flat_result.reshape(inputs_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_scatter(inputs_embeds, special_image_mask, image_features):\n",
    "    # Flatten all arrays\n",
    "    flat_result = inputs_embeds.reshape(-1)\n",
    "    flat_mask = special_image_mask.reshape(-1)\n",
    "    flat_source = image_features.reshape(-1)\n",
    "    \n",
    "    source_idx = 0\n",
    "    for i in range(flat_result.size):\n",
    "        if flat_mask[i]:\n",
    "            flat_result[i] = flat_source[source_idx]\n",
    "            source_idx += 1\n",
    "            \n",
    "            # Wrap around if we've used all source elements\n",
    "            if source_idx >= flat_source.size:\n",
    "                raise Exception(\"Number of elements of source < number of ones in mask\")\n",
    "    # Reshape the result back to the original shape\n",
    "    return flat_result.reshape(inputs_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_scatter(inputs_embeds, special_image_mask, image_features):\n",
    "    # Flatten the arrays\n",
    "    flat_result = np.array(inputs_embeds).ravel()\n",
    "    flat_mask = np.array(special_image_mask).ravel()\n",
    "    flat_source = np.array(image_features).ravel()\n",
    "    \n",
    "    # Get indices where mask is True\n",
    "    indices = np.flatnonzero(flat_mask)\n",
    "    print(type(indices))\n",
    "    \n",
    "    # Check if there are enough elements in flat_source\n",
    "    num_masked = indices.size\n",
    "    if flat_source.size < num_masked:\n",
    "        raise Exception(\"Number of elements of source < number of ones in mask\")\n",
    "    \n",
    "    # Perform the assignment using vectorized operations\n",
    "    flat_result[indices] = flat_source[:num_masked]\n",
    "    \n",
    "    # Reshape the result back to the original shape\n",
    "    return flat_result.reshape(inputs_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self = torch.tensor([[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]])\n",
    "# mask = torch.tensor([[1, 1, 1, 1, 1], [1, 0, 1, 1, 1]], dtype=torch.bool)\n",
    "# source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
    "\n",
    "import torch\n",
    "import mlx.core as mx\n",
    "\n",
    "self = torch.randn(1, 505, 5120)\n",
    "source = torch.randn(1, 475, 5120)\n",
    "mask = torch.randint(0, 2, (1, 505, 5120), dtype=torch.bool)\n",
    "\n",
    "mx_self = mx.array(self.detach().numpy())\n",
    "mx_source = mx.array(source.detach().numpy())\n",
    "mx_mask = mx.array(mask.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = mx.array([1,2,3], dtype=mx.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll[[1,2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = self.masked_scatter(mask, source) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "jp = masked_scatter(mx_self, mx_mask, mx_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(jp == kp.detach().numpy()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = mx.array([1, 2, 3], dtype=mx.bfloat16)\n",
    "lk = mx.array([0, 1, 0], dtype=mx.bool_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko = mx.array(np.flatnonzero(lk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll[ko] = ll[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = mx.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.arange(ll.size)[ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll[mx.array([1,2,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx\n",
    "\n",
    "array = mlx.core.array([True, False, True, True, False])\n",
    "indices = mlx.core.where(array)[0]\n",
    "\n",
    "print(indices)  # Output: [0 2 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[4 096, 224]\t\n",
    "F16\n",
    "\n",
    "model.layers.0.mlp.down_proj.scales\t[4 096, 224]\t\n",
    "F16\n",
    "\n",
    "model.layers.0.mlp.down_proj.weight\t[4 096, 1 792]\t\n",
    "U32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CLANG\"] = \"1\"\n",
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "from tinygrad import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randint(0, 9, size=(1024, 512), dtype=np.uint32)\n",
    "s = np.random.rand(1024, 64).astype(np.float16)\n",
    "b = np.random.rand(1024, 64).astype(np.float16)\n",
    "x = np.random.rand(120, 4096).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def quantized_matmul(x, w_packed, scales, biases, width=4, groups=64):\n",
    "    \"\"\"\n",
    "    Perform quantized matrix multiplication between input x and quantized weights w_packed.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: np.ndarray of shape (M, K), input activations\n",
    "    - w_packed: np.ndarray of shape (N, K_packed), packed quantized weights\n",
    "    - scales: np.ndarray of shape (N, K // groups), scales for dequantization\n",
    "    - biases: np.ndarray of shape (N, K // groups), biases for dequantization\n",
    "    - width: int, number of bits per quantized value (default is 4 bits)\n",
    "    - groups: int, number of quantization groups (default is 64)\n",
    "    \n",
    "    Returns:\n",
    "    - output: np.ndarray of shape (M, N), result of the quantized matrix multiplication\n",
    "    \"\"\"\n",
    "    M, K = x.shape\n",
    "    N, K_packed = w_packed.shape\n",
    "    num_values_per_uint32 = 32 // width\n",
    "    K_unpacked = K_packed * num_values_per_uint32\n",
    "    num_groups = K // groups\n",
    "\n",
    "    assert K == K_unpacked, f\"Mismatch in K dimensions: {K} vs {K_unpacked}\"\n",
    "    assert scales.shape == biases.shape == (N, num_groups), \"Scales and biases must have shape (N, K // groups)\"\n",
    "    assert K % groups == 0, \"K must be divisible by the number of groups\"\n",
    "\n",
    "    # Prepare bitmask and shifts for unpacking\n",
    "    bitmask = (1 << width) - 1\n",
    "    shifts = np.arange(num_values_per_uint32) * width\n",
    "\n",
    "    # Reshape x for group-wise processing\n",
    "    x_grouped = x.reshape(M, num_groups, groups)\n",
    "\n",
    "    # Initialize the output matrix\n",
    "    output = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "    # Process each group\n",
    "    for g in range(num_groups):\n",
    "        # Extract scales and biases for the current group\n",
    "        scale_g = scales[:, g].astype(np.float32)  # Shape: (N,)\n",
    "        bias_g = biases[:, g].astype(np.float32)   # Shape: (N,)\n",
    "\n",
    "        # Calculate the start and end indices for the packed weights of the current group\n",
    "        packs_per_group = groups // num_values_per_uint32  # Number of uint32 packs per group\n",
    "        pack_start = g * packs_per_group\n",
    "        pack_end = pack_start + packs_per_group\n",
    "\n",
    "        # Extract the packed weights for the current group\n",
    "        w_packed_group = w_packed[:, pack_start:pack_end]  # Shape: (N, packs_per_group)\n",
    "\n",
    "        # Unpack the quantized weights on-the-fly\n",
    "        w_quantized_group = np.zeros((N, groups), dtype=np.uint8)  # Shape: (N, groups)\n",
    "        for i, shift in enumerate(shifts):\n",
    "            w_values = (w_packed_group >> shift) & bitmask  # Shape: (N, packs_per_group)\n",
    "            indices = np.arange(i, groups, num_values_per_uint32)\n",
    "            w_quantized_group[:, indices] = w_values\n",
    "\n",
    "        # Dequantize the unpacked weights for the current group\n",
    "        w_group = w_quantized_group.astype(np.float32)\n",
    "        w_group = w_group * scale_g[:, np.newaxis] + bias_g[:, np.newaxis]  # Shape: (N, groups)\n",
    "\n",
    "        # Extract the corresponding input activations for the current group\n",
    "        x_group = x_grouped[:, g, :]  # Shape: (M, groups)\n",
    "\n",
    "        # Perform the partial matrix multiplication and accumulate the results\n",
    "        output += np.dot(x_group, w_group.T)  # Shape: (M, N)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit quantized_matmul(x, w, s, b)\n",
    "quantized_matmul(x, w, s, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = mx.quantized_matmul(mx.array(x), mx.array(w), scales=mx.array(s), biases=mx.array(b), transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor\n",
    "from tinygrad.dtype import dtypes\n",
    "\n",
    "def quantized_matmul_tg(x, w_packed, scales, biases, width=4, groups=64):\n",
    "    \"\"\"\n",
    "    Perform quantized matrix multiplication between input x and quantized weights w_packed using tinygrad Tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    - x: Tensor of shape (M, K), input activations\n",
    "    - w_packed: Tensor of shape (N, K_packed), packed quantized weights (dtype=Tensor.int32)\n",
    "    - scales: Tensor of shape (N, K // groups), scales for dequantization (dtype=Tensor.float32)\n",
    "    - biases: Tensor of shape (N, K // groups), biases for dequantization (dtype=Tensor.float32)\n",
    "    - width: int, number of bits per quantized value (default is 4 bits)\n",
    "    - groups: int, number of quantization groups (default is 64)\n",
    "    \n",
    "    Returns:\n",
    "    - output: Tensor of shape (M, N), result of the quantized matrix multiplication\n",
    "    \"\"\"\n",
    "    M, K = x.shape\n",
    "    N, K_packed = w_packed.shape\n",
    "    num_values_per_uint32 = 32 // width\n",
    "    K_unpacked = K_packed * num_values_per_uint32\n",
    "    num_groups = K // groups\n",
    "\n",
    "    assert K == K_unpacked, f\"Mismatch in K dimensions: {K} vs {K_unpacked}\"\n",
    "    assert scales.shape == (N, num_groups), f\"Scales must have shape (N, {num_groups}), but is {scales.shape}\"\n",
    "    assert biases.shape == (N, num_groups), f\"Biases must have shape (N, {num_groups}), but is {biases.shape}\"\n",
    "    assert K % groups == 0, \"K must be divisible by the number of groups\"\n",
    "\n",
    "    # Prepare bitmask and shifts for unpacking\n",
    "    bitmask = (1 << width) - 1  # e.g., for width=4, bitmask=0b1111\n",
    "    shifts = Tensor.arange(num_values_per_uint32, dtype=dtypes.uint32) * width  # Tensor of shifts\n",
    "\n",
    "    packs_per_group = groups // num_values_per_uint32  # Number of uint32 packs per group\n",
    "\n",
    "    # Reshape x for group-wise processing\n",
    "    x_grouped = x.reshape(M, num_groups, groups)\n",
    "\n",
    "    # Initialize the output matrix\n",
    "    output = Tensor.zeros((M, N), dtype=dtypes.float32)\n",
    "\n",
    "    # Process each group\n",
    "    for g in range(num_groups):\n",
    "        # Extract scales and biases for the current group\n",
    "        scale_g = scales[:, g]  # Shape: (N,)\n",
    "        bias_g = biases[:, g]   # Shape: (N,)\n",
    "\n",
    "        # Calculate the start and end indices for the packed weights of the current group\n",
    "        pack_start = g * packs_per_group\n",
    "        pack_end = pack_start + packs_per_group\n",
    "\n",
    "        # Extract the packed weights for the current group\n",
    "        w_packed_group = w_packed[:, pack_start:pack_end]  # Shape: (N, packs_per_group)\n",
    "        \n",
    "        # Unpack the quantized weights on-the-fly\n",
    "        w_quantized_group = Tensor.zeros((N, groups), dtype=dtypes.uint8)  # Shape: (N, groups)\n",
    "        print(w_quantized_group.shape)\n",
    "        for i, shift in enumerate(shifts):\n",
    "            w_values = (w_packed_group >> shift.item()) & bitmask  # Shape: (N, packs_per_group)\n",
    "            # w_values.squee\n",
    "            print(w_values.numpy().shape)\n",
    "            indices = Tensor.arange(i, groups, num_values_per_uint32)\n",
    "            print(indices.shape)\n",
    "            w_quantized_group[:, indices] = w_values\n",
    "\n",
    "        # Dequantize the unpacked weights for the current group\n",
    "        w_group = w_quantized_group.astype(np.float32)\n",
    "        w_group = w_group * scale_g[:, np.newaxis] + bias_g[:, np.newaxis]  # Shape: (N, groups)\n",
    "\n",
    "        # Extract the corresponding input activations for the current group\n",
    "        x_group = x_grouped[:, g, :]  # Shape: (M, groups)\n",
    "\n",
    "        # Perform the partial matrix multiplication and accumulate the results\n",
    "        output += x_group.dot(w_group.transpose())  # Shape: (M, N)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_matmul_tg(Tensor(x), Tensor(w), Tensor(s), Tensor(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = Tensor.arange(32, dtype=dtypes.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ll*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = np.arange(8) * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shift in shifts:\n",
    "    print(type(shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = Tensor.arange(32, dtype=dtypes.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = ll >> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(32) >> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = Tensor.randint((3, 2)).numpy()\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.dtype import dtypes\n",
    "\n",
    "def quantized_matmul_tg(x, w_packed, scales, biases, width=4, groups=64):\n",
    "    \"\"\"\n",
    "    Perform quantized matrix multiplication using tinygrad Tensors with shift operators.\n",
    "\n",
    "    Parameters:\n",
    "    - x: Tensor of shape (M, K), input activations.\n",
    "    - w_packed: Tensor of shape (N, K_packed), packed quantized weights (dtype=dtypes.int32).\n",
    "    - scales: Tensor of shape (N, K // groups), scales for dequantization (dtype=dtypes.float32).\n",
    "    - biases: Tensor of shape (N, K // groups), biases for dequantization (dtype=dtypes.float32).\n",
    "    - width: int, number of bits per quantized value (default is 4 bits).\n",
    "    - groups: int, number of quantization groups (default is 64).\n",
    "\n",
    "    Returns:\n",
    "    - output: Tensor of shape (M, N), result of the quantized matrix multiplication.\n",
    "    \"\"\"\n",
    "    M, K = x.shape\n",
    "    N, K_packed = w_packed.shape\n",
    "\n",
    "    num_values_per_uint32 = 32 // width  # E.g., for width=4, this is 8\n",
    "    K_unpacked = K_packed * num_values_per_uint32\n",
    "    num_groups = K // groups\n",
    "    packs_per_group = groups // num_values_per_uint32  # Number of uint32 packs per group\n",
    "\n",
    "    assert K == K_unpacked, f\"Mismatch in K dimensions: {K} vs {K_unpacked}\"\n",
    "    assert scales.shape == (N, num_groups), f\"Scales must have shape (N, {num_groups}), got {scales.shape}\"\n",
    "    assert biases.shape == (N, num_groups), f\"Biases must have shape (N, {num_groups}), got {biases.shape}\"\n",
    "    assert K % groups == 0, \"K must be divisible by the number of groups\"\n",
    "\n",
    "    # Prepare bitmask\n",
    "    bitmask = (1 << width) - 1  # E.g., for width=4, bitmask=15\n",
    "\n",
    "    # Reshape x for group-wise processing\n",
    "    x_grouped = x.reshape(M, num_groups, groups)  # Shape: (M, num_groups, groups)\n",
    "\n",
    "    # Initialize the output matrix\n",
    "    output = Tensor.zeros((M, N), dtype=dtypes.float16)\n",
    "\n",
    "    # Prepare shift amounts\n",
    "    shift_list = [i * width for i in range(num_values_per_uint32)]\n",
    "\n",
    "    # Process each group\n",
    "    for g in range(num_groups):\n",
    "        # Extract scales and biases for the current group\n",
    "        scale_g = scales[:, g].reshape(N, 1)  # Shape: (N, 1)\n",
    "        bias_g = biases[:, g].reshape(N, 1)   # Shape: (N, 1)\n",
    "\n",
    "        # Extract the packed weights for the current group\n",
    "        pack_start = g * packs_per_group\n",
    "        pack_end = pack_start + packs_per_group\n",
    "        w_packed_group = w_packed[:, pack_start:pack_end]  # Shape: (N, packs_per_group)\n",
    "\n",
    "        # Initialize a list to collect unpacked values\n",
    "        unpacked_values = []\n",
    "\n",
    "        # Unpack the quantized weights\n",
    "        for shift_amount in shift_list:\n",
    "            # Perform the shift and mask operations\n",
    "            shifted = w_packed_group >> shift_amount  # Broadcasting scalar shift_amount\n",
    "            masked = (shifted & bitmask).cast(dtypes.float16)\n",
    "            masked = masked.reshape(N, -1)  # Flatten over packs_per_group\n",
    "\n",
    "            unpacked_values.append(masked)\n",
    "\n",
    "        # Stack the unpacked values and transpose to get correct order\n",
    "        # After stacking: Shape becomes (num_values_per_uint32, N, total_packed_values)\n",
    "        w_unpacked_stack = Tensor.stack(*unpacked_values, dim=0)\n",
    "        w_unpacked_group = w_unpacked_stack.permute(1, 2, 0).reshape(N, groups)  # Shape: (N, groups)\n",
    "\n",
    "        # Dequantize the unpacked weights\n",
    "        w_group = w_unpacked_group * scale_g + bias_g  # Shape: (N, groups)\n",
    "\n",
    "        # Extract the input activations for the current group\n",
    "        x_group = x_grouped[:, g, :]  # Shape: (M, groups)\n",
    "\n",
    "        # Perform matrix multiplication and accumulate the result\n",
    "        partial_output = x_group @ w_group.transpose()  # Shape: (M, N)\n",
    "        output += partial_output\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = quantized_matmul_tg(Tensor(x).realize(), Tensor(w).realize(), Tensor(s).realize(), Tensor(b).realize()).realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.quantized_matmul(mx.array(x), mx.array(w), mx.array(s), mx.array(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor.zeros((120, 1024)).cast(dtypes.int32).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor.zeros((120, 1024)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=100\n",
    "lm[i][j], kp[i][j].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = Tensor.randint((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll.cat(Tensor.empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = [Tensor([[1, 2]]), Tensor([[3, 4]]), Tensor([[5, 6]])]\n",
    "\n",
    "# Concatenate along dimension 0\n",
    "result = Tensor.cat(*tensors[1:], dim=1)\n",
    "print(result.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = Tensor.zeros((120, 1024))\n",
    "ll.realize().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = Tensor.ones(2, 2, dtype=dtypes.uint32).realize()\n",
    "lm = Tensor.ones(2, 2, dtype=dtypes.float32).realize()\n",
    "\n",
    "kp = ll@lm.realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4096/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CLANG\"] = \"1\"\n",
    "\n",
    "from tinygrad import Tensor\n",
    "from tinygrad.dtype import dtypes\n",
    "\n",
    "class MLXLinear:\n",
    "  def __init__(self, in_features, out_features, bits=4, group_size=64, bias=False):\n",
    "    assert in_features % group_size == 0\n",
    "    assert 32 % bits == 0\n",
    "    assert (in_features * bits) % 32 == 0\n",
    "    self.weight = Tensor.ones(out_features, (in_features * bits) // 32, dtype=dtypes.uint32)\n",
    "    self.scales = Tensor.ones(out_features, in_features // group_size, dtype=dtypes.half)\n",
    "    if bias:\n",
    "      self.biases = Tensor.ones(out_features, in_features // group_size, dtype=dtypes.half)\n",
    "    self.bits = bits\n",
    "    self.group_size = group_size\n",
    "\n",
    "  def __call__(self, x):\n",
    "    M, K = x.shape\n",
    "    N, K_packed = self.weight.shape\n",
    "\n",
    "    num_values_per_uint32 = 32 // self.bits\n",
    "    K_unpacked = K_packed * num_values_per_uint32\n",
    "    num_groups = K // self.group_size\n",
    "    packs_per_group = self.group_size // num_values_per_uint32\n",
    "\n",
    "    assert K == K_unpacked, f\"Mismatch in K dimensions: {K} vs {K_unpacked}\"\n",
    "    assert self.scales.shape == self.biases.shape == (N, num_groups), f\"Scales must have shape (N, {num_groups}), got {self.scales.shape}\"\n",
    "    assert K % self.group_size == 0, \"K must be divisible by the number of groups\"\n",
    "\n",
    "    bitmask = (1 << self.bits) - 1\n",
    "\n",
    "    x_grouped = x.reshape(M, num_groups, self.group_size)\n",
    "\n",
    "    output = Tensor.zeros((M, N), dtype=dtypes.float16)\n",
    "\n",
    "    shift_list = [i * self.bits for i in range(num_values_per_uint32)]\n",
    "\n",
    "    for g in range(num_groups):\n",
    "        scale_g = self.scales[:, g].reshape(N, 1)\n",
    "        bias_g = self.biases[:, g].reshape(N, 1)\n",
    "\n",
    "        pack_start = g * packs_per_group\n",
    "        pack_end = pack_start + packs_per_group\n",
    "        w_packed_group = self.weight[:, pack_start:pack_end]\n",
    "\n",
    "        unpacked_values = []\n",
    "\n",
    "        for shift_amount in shift_list:\n",
    "            shifted = w_packed_group >> shift_amount\n",
    "            masked = (shifted & bitmask).cast(dtypes.float16)\n",
    "            masked = masked.reshape(N, -1)\n",
    "\n",
    "            unpacked_values.append(masked)\n",
    "\n",
    "        w_unpacked_stack = Tensor.stack(*unpacked_values, dim=0)\n",
    "        w_unpacked_group = w_unpacked_stack.permute(1, 2, 0).reshape(N, self.group_size)\n",
    "        w_group = w_unpacked_group * scale_g + bias_g\n",
    "\n",
    "        x_group = x_grouped[:, g, :]\n",
    "\n",
    "        partial_output = x_group @ w_group.T\n",
    "        output += partial_output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlx.nn as nn\n",
    "import mlx.core as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = weights[\"model.layers.0.self_attn.q_proj.weight\"]\n",
    "s = weights[\"model.layers.0.self_attn.q_proj.scales\"]\n",
    "b = weights[\"model.layers.0.self_attn.q_proj.biases\"]\n",
    "x = np.random.rand(120, 4096).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "weights = {}\n",
    "weights.update(mx.load(\"./../model.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in weights.keys():\n",
    "    if \"embed\" in l:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[\"model.embed_tokens.scales\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[\"model.embed_tokens.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jp = nn.QuantizedLinear(4096, 4096, bias=False)\n",
    "jp.weight = w\n",
    "jp.scales = s\n",
    "jp.biases = b\n",
    "jp(mx.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import nn as nn1\n",
    "\n",
    "nn1.Linear(4096, 4096)(Tensor(x).realize()).realize().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = MLXLinear(4096, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp.weight = Tensor(np.array(w)).realize()\n",
    "kp.scales = Tensor(np.array(s)).realize()\n",
    "kp.biases = Tensor(np.array(b)).realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = kp(Tensor(x).realize()).realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_matmul(x, np.array(w), np.array(s), np.array(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "# model_id = \"mistral-community/pixtral-12b\"\n",
    "# model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model_id = \"varb15/hf-internal-testing-pixtral-12b\"\n",
    "# processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "url_dog = \"https://picsum.photos/id/237/200/300\"\n",
    "url_mountain = \"https://picsum.photos/seed/picsum/200/300\"\n",
    "\n",
    "# chat = [\n",
    "#     {\n",
    "#       \"role\": \"system\", \"content\": \"haha\"\n",
    "#     },\n",
    "#     {\n",
    "#       \"role\": \"user\", \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": \"Can this animal\"}, \n",
    "#         {\"type\": \"image\", \"image\": \"haha\"}, \n",
    "#         {\"type\": \"text\", \"text\": \"live here?\"}, \n",
    "#         {\"type\": \"image\"}\n",
    "#       ]\n",
    "#     },\n",
    "#     {\n",
    "#       \"role\": \"user\", \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": \"Can this animal\"}, \n",
    "#         {\"type\": \"image\", \"image\": \"haha\"}, \n",
    "#         {\"type\": \"text\", \"text\": \"live here?\"}, \n",
    "#         {\"type\": \"image\"}\n",
    "#       ]\n",
    "#     },\n",
    "#     {\n",
    "#       \"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"Can this animal\"}]\n",
    "#     },\n",
    "#     {\n",
    "#       \"role\": \"user\", \"content\": [\n",
    "#         {\"type\": \"text\", \"text\": \"Can this animal\"}, \n",
    "#         {\"type\": \"image\", \"image\": \"haha\"}, \n",
    "#         {\"type\": \"text\", \"text\": \"live here?\"}, \n",
    "#         {\"type\": \"image\"}\n",
    "#       ]\n",
    "#     }\n",
    "# ]\n",
    "# chat = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"image\"}, {\"type\": \"text\", \"text\": \"explain\"}]}, {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \" The image displays a computer screen with multiple graphs and charts. These graphs and charts are likely used for monitoring and analyzing various data points. The graphs show different types of data, such as CPU usage, memory usage, and network traffic. The charts are organized in a way that allows for easy comparison and understanding of the data. The screen is filled with a variety of graphs and charts, indicating a complex system being monitored and analyzed.\"}]}, {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"but like explain properly, be more clear\"}]}]\n",
    "# chat = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"image\"}, {\"type\": \"text\", \"text\": \"explain\"}]}, {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \" The image displays a computer screen with multiple graphs and charts. These graphs and charts are likely used for monitoring and analyzing various data points. The graphs show different types of data, such as CPU usage, memory usage, and network traffic. The charts are organized in a way that allows for easy comparison and understanding of the data. The screen is filled with a variety of graphs and charts, indicating a complex system being monitored and analyzed.\"}]}, {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"but like explain properly, be more clear\"}]}]\n",
    "# chat = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": \"image\"}, {\"type\": \"text\", \"text\": \"explain\"}]}, {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"explain\"}]}]\n",
    "# chat = [{\"role\": \"user\", \"content\": \"hi\"}]\n",
    "chat = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"hi\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"Hello! How can I assist you today? Let's chat about anything you'd like. ���\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"ok how can you help?\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"I can help in a variety of ways! Here are some things I can do:\\n\\n1. **Answer Questions**: Provide information on a wide range of topics, from general knowledge to specific queries.\\n2. **Explain Concepts**: Break down complex ideas into simpler parts to help you understand them better.\\n3. **Offer Suggestions**: Provide recommendations for books, movies, recipes, and more.\\n4. **Help with Language**: Assist with grammar, vocabulary, or translations in multiple languages.\\n5. **Provide Study Help**: Offer tips, summaries, and explanations for various subjects.\\n6. **Engage in Conversation**: Chat on various topics to help you practice a language or just have a friendly chat.\\n7. **Offer Tips and Advice**: Provide advice on productivity, wellness, and other lifestyle topics.\\n\\nWhat do you need help with today?\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      {\n",
    "        \"type\": \"image\",\n",
    "        \"image\": \"image\"\n",
    "      },\n",
    "      {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"explain this image please?\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "prompt = processor.apply_chat_template(chat)\n",
    "# inputs = processor(text=prompt, images=[url_dog, url_mountain], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST]hi[/INST]Hello! How can I assist you today? Let's chat about anything you'd like. ���</s>[INST]ok how can you help?[/INST]I can help in a variety of ways! Here are some things I can do:\\n\\n1. **Answer Questions**: Provide information on a wide range of topics, from general knowledge to specific queries.\\n2. **Explain Concepts**: Break down complex ideas into simpler parts to help you understand them better.\\n3. **Offer Suggestions**: Provide recommendations for books, movies, recipes, and more.\\n4. **Help with Language**: Assist with grammar, vocabulary, or translations in multiple languages.\\n5. **Provide Study Help**: Offer tips, summaries, and explanations for various subjects.\\n6. **Engage in Conversation**: Chat on various topics to help you practice a language or just have a friendly chat.\\n7. **Offer Tips and Advice**: Provide advice on productivity, wellness, and other lifestyle topics.\\n\\nWhat do you need help with today?</s>[INST][IMG]explain this image please?[/INST]\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/varb/pixtral_temp/processor_config.json']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.save_pretrained(\"/Users/varb/pixtral_temp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = \"\"\"{%- if messages[0][\\\"role\\\"] == \\\"system\\\" %}\\n  {%- set system_message = messages[0][\\\"content\\\"] %}\\n  {%- set loop_messages = messages[1:] %}\\n{%- else %}\\n  {%- set loop_messages = messages %}\\n{%- endif %}\\n\\n{{- bos_token }}\\n{%- for message in loop_messages %}\\n  {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\\n    {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\\n  {%- endif %}\\n  {%- if message[\\\"role\\\"] == \\\"user\\\" %}\\n    {%- if loop.last and system_message is defined %}\\n      {{- \\\"[INST]\\\" + system_message + \\\"\\\\n\\\\n\\\" }}\\n    {%- else %}\\n      {{- \\\"[INST]\\\" }}\\n    {%- endif %}\\n    {%- if message[\\\"content\\\"] is not string %}\\n      {%- for chunk in message[\\\"content\\\"] %}\\n        {%- if chunk[\\\"type\\\"] == \\\"text\\\" %}\\n          {{- chunk[\\\"text\\\"] }}\\n        {%- elif chunk[\\\"type\\\"] == \\\"image\\\" %}\\n          {{- \\\"[IMG]\\\" }}\\n        {%- else %}\\n          {{- raise_exception(\\\"Unrecognized content type!\\\") }}\\n        {%- endif %}\\n      {%- endfor %}\\n    {%- else %}\\n      {{- message[\\\"content\\\"] }}\\n    {%- endif %}\\n    {{- \\\"[/INST]\\\" }}\\n  {%- elif message[\\\"role\\\"] == \\\"assistant\\\" %}\\n    {%- if message[\\\"content\\\"] is not string %}\\n      {%- for chunk in message[\\\"content\\\"] %}\\n        {%- if chunk[\\\"type\\\"] == \\\"text\\\" %}\\n          {{- chunk[\\\"text\\\"] }}\\n        {%- elif chunk[\\\"type\\\"] == \\\"image\\\" %}\\n          {{- \\\"[IMG]\\\" }}\\n        {%- else %}\\n          {{- raise_exception(\\\"Unrecognized content type!\\\") }}\\n        {%- endif %}\\n      {%- endfor %}\\n    {%- else %}\\n      {{- message[\\\"content\\\"] }}\\n    {%- endif %}\\n    {{- eos_token}}\\n  {%- else %}\\n    {{- raise_exception(\\\"Only user and assistant roles are supported, with the exception of an initial optional system message!\\\") }}\\n  {%- endif %}\\n{%- endfor %}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.chat_template = ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.QuantizedEmbedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exo-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
